# Given the two RDDs: 
#      a. x created from the ordered pairs: ("spark", 1) and ("hadoop", 4) 
#      b. y created from the ordered pairs: ("spark", 2), ("hadoop", 5). 
# Perform the join operation on the RDDs created above, and print the resulting RDD.


!pip install pyspark
from pyspark import SparkContext
from pyspark.sql.session import SparkSession
sc=SparkContext.getOrCreate()
spark=SparkSession(sc)
rdd1=sc.parallelize([("spark",1),("hadoop",4)])
rdd2=sc.parallelize([("spark",2),("hadoop",5)])
rdd=sorted(rdd1.join(rdd2).collect())
print(rdd)
