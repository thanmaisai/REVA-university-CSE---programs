Spark:

Spark is an open-source distributed computing system that is designed to process large volumes of data in parallel across a cluster of machines. It provides a unified API for working with different kinds of data, including structured, semi-structured, and unstructured data, and supports a variety of programming languages, including Python, Java, Scala, and R.

PySpark:

PySpark is the Python API for Spark, which allows you to write Spark programs using Python. It provides a simple and concise programming interface for working with Spark, and allows you to leverage the full power of Spark using Python.

Parallelize() function:

The parallelize() function in PySpark is used to create a new RDD (Resilient Distributed Dataset) from a list or array of objects. It distributes the data across the nodes in the Spark cluster, allowing for parallel processing of the data.

RDD:

RDD stands for Resilient Distributed Dataset, which is the fundamental data structure in Spark. RDDs are immutable distributed collections of objects that can be processed in parallel across a cluster of machines. They can be created from data stored in Hadoop Distributed File System (HDFS), from data stored in other file formats, or from data generated by parallelizing a collection of objects using the parallelize() function.

PySpark DataFrames:

PySpark DataFrames are a high-level abstraction for working with structured data in Spark. They provide a DataFrame API that is similar to the one in Pandas, allowing you to perform operations like filtering, grouping, and aggregating on large datasets in a distributed manner.

Accumulator() function:

The accumulator() function in PySpark is used to create a shared variable that can be used to accumulate values across multiple tasks in a Spark cluster. It is used to implement counters, sums, and other aggregations that require global state.

df.describe().show():

The df.describe().show() function is used to generate summary statistics for a PySpark DataFrame. It returns a new DataFrame that contains the count, mean, standard deviation, minimum, and maximum values for each column in the input DataFrame. The show() function is then used to display the results in the console.

Python Lambda function: 

lambda function is a small anonymous function, it can take any number of arguments, but can only have one expression.

flatMap():

In PySpark, the flatMap() is defined as the transformation operation which flattens the Resilient Distributed Dataset or DataFrame(i.e. array/map DataFrame columns) after applying the function on every element and further returns the new PySpark Resilient Distributed Dataset or DataFrame



Q) What is difference between map and flatMap in PySpark?

map and flatMap are similar, in the sense they take a line from the input RDD and apply a function on it. The way they differ is that the function in map returns only one element, while function in flatMap can return a list of elements (0 or more) as an iterator. Also, the output of the flatMap is flattened.
