!pip install pyspark
from pyspark import SparkContext 
sc=SparkContext.getOrCreate()
words=sc.parallelize(["scala","java","hadoop","spark","akka","spark vs hadoop","pyspark","pyspark and spark"])
count = words.count()
spark_words=words.filter(lambda x:"spark" in x)
print("words containing spark")
for word in spark_words.collect():
  print(word)
