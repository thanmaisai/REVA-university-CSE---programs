# Perform the following operations: 
#        a. Create a SparkContext object 
#        b. Create an RDD from set of words ("scala", "java", "hadoop", "spark", "akka",   "spark vs hadoop", “pyspark", "pyspark and spark") using parallelize() function. 
#        c. Find the total count of the words in the RDD 
#        d. Filter out and print the strings containing the word “spark” from the RDD

!pip install pyspark
from pyspark import SparkContext 
sc=SparkContext.getOrCreate()
words=sc.parallelize(["scala","java","hadoop","spark","akka","spark vs hadoop","pyspark","pyspark and spark"])
count = words.count()
spark_words=words.filter(lambda x:"spark" in x)
print("words containing spark")
for word in spark_words.collect():
  print(word)
